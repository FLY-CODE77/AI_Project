{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection in Keras \n",
    "\n",
    "- Anomaly Detection is unsupervised training tech \n",
    "- Analyzes the degree to which incoming data is different than data that you used to train the neural network\n",
    "- Cybersecurity used anomaly detection to ensure network security \n",
    "\n",
    "---\n",
    "## In Game \n",
    "- Already so many company use Anomaly Detection finding aka. GameBot\n",
    "\n",
    "## **google cloud tech presentation** - in May 10, 2019\n",
    "- Netmable\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('xrZ9BRK5WAg', width=600, height=480)\n",
    "\n",
    "<img src=\"https://blog.kakaocdn.net/dn/2EwaL/btq60IMBfzV/2TWCr5YyVhHWLjRrj2P3T1/img.png\" align =\"left\">\n",
    "\n",
    "- 기존 규칙 기반의 봇 탐지 보다 확실히 좋은.. 성능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Autoencoder을 통한 Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Autoencoder : 자기학습 모델}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Autoencoder의 경우 보통 이미지의 생성이나 복원에 많이 사용되며 \n",
    "- 구조를 이어받아 대표적인 딥러닝 생성 모델인 GAN(Generative Adversarial Network)으로 까지 이어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LSTM Autoencoder 학습 시에는 정상(normal) 신호의 데이터로만 모델을 학습 시킨다**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoder와 decoder는 학습이 진행될 수록 정상 신호를 정산 신호 답게 표현 하는 방법을 학습하고\n",
    "- 최종적으로 재 구성한 결과도 정산 신호와 유사한 분포를 가지는 데이터!!가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jaehyeongan.github.io/image/lstm-autoencoder-architecture2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만약에 비 정상 신호를 입력에 넣게 된다면 정상 분포와 다른 특성의 분포를 나타낼것이기에 높은 reconstruction error가 생길것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold by precision-Recall-Curve\n",
    "- Autoencoder에서 재구성된 결과 vs input를 비교 하여서 Reconstruction error를 계산하는데\n",
    "- error val 이 낮다면 정상, 높다면 비정상으로 판단하기로 하였다\n",
    "- 그러면 이 판단 기준은 어떻게 짜야 할까!?\n",
    "\n",
    "\n",
    "- 보통 classification 같은 경우는 50%를 기준으로 판단을 하지만\n",
    "- Reconstruction error 같은 경우는 극단적으로 값이 튀기 어렵기에.. \n",
    "- 타당한 threshold 값을 정하는 것이 필요하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision Recall Val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jaehyeongan.github.io/image/precision-recall-curve.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- recall과 precision이 tradoff 관계인 점을 이용해서 한쪽에 치우치는 않는 케이스를 찾기 위한 과정으로 \n",
    "- precision recall curve를 사용해준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KDD99 DATA SET\n",
    "- Old dataset 20 years old \n",
    "- Still widely used to demonstarate intrusion Detection system \n",
    "- Used for international knowledge Discovery and Data Mining Tools Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- build a network intrusion detector **침입 감지**\n",
    "    - predictive model capable of distinguishing between **\"bad\"**, **\"good\"** connection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This database contains a standard set of data to be audited,\n",
    "- including a wide variety of intrusions simulated in a military network environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note \n",
    "- kdd99 does not include column names, caus to old "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/flycode77/.keras/datasets/kddcup.data_10_percent.gz\n",
      "Read 494021 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494019</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494020</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494021 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration protocol_type  ... dst_host_srv_rerror_rate  outcome\n",
       "0              0           tcp  ...                      0.0  normal.\n",
       "1              0           tcp  ...                      0.0  normal.\n",
       "...          ...           ...  ...                      ...      ...\n",
       "494019         0           tcp  ...                      0.0  normal.\n",
       "494020         0           tcp  ...                      0.0  normal.\n",
       "\n",
       "[494021 rows x 42 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "pd.set_option('display.max_columns', 6)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "\n",
    "try:\n",
    "    path = get_file('kddcup.data_10_percent.gz', origin=\\\n",
    "    'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz')\n",
    "except:\n",
    "    print('Error downloading')\n",
    "    raise\n",
    "    \n",
    "print(path) \n",
    "\n",
    "df = pd.read_csv(path, header=None)\n",
    "\n",
    "print(\"Read {} rows.\".format(len(df)))\n",
    "\n",
    "df.dropna(inplace=True,axis=1) \n",
    "\n",
    "df.columns = [\n",
    "    'duration',\n",
    "    'protocol_type',\n",
    "    'service',\n",
    "    'flag',\n",
    "    'src_bytes',\n",
    "    'dst_bytes',\n",
    "    'land',\n",
    "    'wrong_fragment',\n",
    "    'urgent',\n",
    "    'hot',\n",
    "    'num_failed_logins',\n",
    "    'logged_in',\n",
    "    'num_compromised',\n",
    "    'root_shell',\n",
    "    'su_attempted',\n",
    "    'num_root',\n",
    "    'num_file_creations',\n",
    "    'num_shells',\n",
    "    'num_access_files',\n",
    "    'num_outbound_cmds',\n",
    "    'is_host_login',\n",
    "    'is_guest_login',\n",
    "    'count',\n",
    "    'srv_count',\n",
    "    'serror_rate',\n",
    "    'srv_serror_rate',\n",
    "    'rerror_rate',\n",
    "    'srv_rerror_rate',\n",
    "    'same_srv_rate',\n",
    "    'diff_srv_rate',\n",
    "    'srv_diff_host_rate',\n",
    "    'dst_host_count',\n",
    "    'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate',\n",
    "    'outcome'\n",
    "]\n",
    "\n",
    "# display 5 rows\n",
    "pd.set_option('display.max_columns', 5)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"outcome\" specifies either \"normal,\" indicating no attack, or the type of attack performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outcome\n",
       "back.               2203\n",
       "buffer_overflow.      30\n",
       "                    ... \n",
       "warezclient.        1020\n",
       "warezmaster.          20\n",
       "Name: outcome, Length: 23, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('outcome')[\"outcome\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before MLP learn need some processing \n",
    "    - 1. The first function converts numeric columns into Z-Scores. # 정규화\n",
    "    - 2. The second function replaces categorical values with dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "    \n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] \n",
    "# for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>...</th>\n",
       "      <th>is_host_login-0</th>\n",
       "      <th>is_guest_login-0</th>\n",
       "      <th>is_guest_login-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.067792</td>\n",
       "      <td>-0.002879</td>\n",
       "      <td>0.138664</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.067792</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>-0.011578</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.067792</td>\n",
       "      <td>-0.002824</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.067792</td>\n",
       "      <td>-0.002840</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.067792</td>\n",
       "      <td>-0.002842</td>\n",
       "      <td>0.035214</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  src_bytes  dst_bytes  ...  is_host_login-0  is_guest_login-0  \\\n",
       "0 -0.067792  -0.002879   0.138664  ...                1                 1   \n",
       "1 -0.067792  -0.002820  -0.011578  ...                1                 1   \n",
       "2 -0.067792  -0.002824   0.014179  ...                1                 1   \n",
       "3 -0.067792  -0.002840   0.014179  ...                1                 1   \n",
       "4 -0.067792  -0.002842   0.035214  ...                1                 1   \n",
       "\n",
       "   is_guest_login-1  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now encode the feature vector\n",
    "\n",
    "pd.set_option('display.max_columns', 6)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "\n",
    "encode_numeric_zscore(df, 'duration')\n",
    "encode_text_dummy(df, 'protocol_type')\n",
    "encode_text_dummy(df, 'service')\n",
    "encode_text_dummy(df, 'flag')\n",
    "encode_numeric_zscore(df, 'src_bytes')\n",
    "encode_numeric_zscore(df, 'dst_bytes')\n",
    "encode_text_dummy(df, 'land')\n",
    "encode_numeric_zscore(df, 'wrong_fragment')\n",
    "encode_numeric_zscore(df, 'urgent')\n",
    "encode_numeric_zscore(df, 'hot')\n",
    "encode_numeric_zscore(df, 'num_failed_logins')\n",
    "encode_text_dummy(df, 'logged_in')\n",
    "encode_numeric_zscore(df, 'num_compromised')\n",
    "encode_numeric_zscore(df, 'root_shell')\n",
    "encode_numeric_zscore(df, 'su_attempted')\n",
    "encode_numeric_zscore(df, 'num_root')\n",
    "encode_numeric_zscore(df, 'num_file_creations')\n",
    "encode_numeric_zscore(df, 'num_shells')\n",
    "encode_numeric_zscore(df, 'num_access_files')\n",
    "encode_numeric_zscore(df, 'num_outbound_cmds')\n",
    "encode_text_dummy(df, 'is_host_login')\n",
    "encode_text_dummy(df, 'is_guest_login')\n",
    "encode_numeric_zscore(df, 'count')\n",
    "encode_numeric_zscore(df, 'srv_count')\n",
    "encode_numeric_zscore(df, 'serror_rate')\n",
    "encode_numeric_zscore(df, 'srv_serror_rate')\n",
    "encode_numeric_zscore(df, 'rerror_rate')\n",
    "encode_numeric_zscore(df, 'srv_rerror_rate')\n",
    "encode_numeric_zscore(df, 'same_srv_rate')\n",
    "encode_numeric_zscore(df, 'diff_srv_rate')\n",
    "encode_numeric_zscore(df, 'srv_diff_host_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_count')\n",
    "encode_numeric_zscore(df, 'dst_host_srv_count')\n",
    "encode_numeric_zscore(df, 'dst_host_same_srv_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_diff_srv_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_same_src_port_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_srv_diff_host_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_serror_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_srv_serror_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_rerror_rate')\n",
    "encode_numeric_zscore(df, 'dst_host_srv_rerror_rate')\n",
    "\n",
    "# display 5 rows\n",
    "\n",
    "df.dropna(inplace=True,axis=1)\n",
    "df[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO ANOMALY DETECTION \n",
    "- Perform anomaly detection, \n",
    "- Data need to divide by two grouop (\"Normal\", \"various Attack \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count: 97278\n",
      "Attack count: 396743\n"
     ]
    }
   ],
   "source": [
    "normal_mask = df['outcome']=='normal.'\n",
    "attack_mask = df['outcome']!='normal.'\n",
    "\n",
    "df.drop('outcome',axis=1,inplace=True)\n",
    "\n",
    "df_normal = df[normal_mask]\n",
    "df_attack = df[attack_mask]\n",
    "\n",
    "print(f\"Normal count: {len(df_normal)}\")\n",
    "print(f\"Attack count: {len(df_attack)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame into Numpy arrays, keras need numpy\n",
    "x_normal = df_normal.values\n",
    "x_attack = df_attack.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- important to note that we are not using the outcome column as a label to predict\n",
    "- This anomaly detection is unsupervised\n",
    "- **no target (y)**\n",
    "- train an autoencoder on the normal data \n",
    "- see how well it can detect that the data not flagged as \"normal\" represents an anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_normal_train, x_normal_test = train_test_split(\n",
    "    x_normal, test_size=0.3, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal train count: 68094\n",
      "Normal test count: 29184\n"
     ]
    }
   ],
   "source": [
    "print(f\"Normal train count: {len(x_normal_train)}\")\n",
    "print(f\"Normal test count: {len(x_normal_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The autoencoder will learn to compress the data to a vector of just three numbers\n",
    "- autoencoder should be able to also decompress with reasonable accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "68094/68094 [==============================] - 2s 35us/sample - loss: 0.4558\n",
      "Epoch 2/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.3404\n",
      "Epoch 3/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.2653\n",
      "Epoch 4/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.2524\n",
      "Epoch 5/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1907\n",
      "Epoch 6/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1941\n",
      "Epoch 7/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1948\n",
      "Epoch 8/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1834\n",
      "Epoch 9/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1993\n",
      "Epoch 10/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1783\n",
      "Epoch 11/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1800\n",
      "Epoch 12/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1851\n",
      "Epoch 13/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1778\n",
      "Epoch 14/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1832\n",
      "Epoch 15/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1685\n",
      "Epoch 16/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1876\n",
      "Epoch 17/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1767\n",
      "Epoch 18/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1707\n",
      "Epoch 19/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1622\n",
      "Epoch 20/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1699\n",
      "Epoch 21/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1591\n",
      "Epoch 22/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1658\n",
      "Epoch 23/100\n",
      "68094/68094 [==============================] - 1s 21us/sample - loss: 0.1671\n",
      "Epoch 24/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1619\n",
      "Epoch 25/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1485\n",
      "Epoch 26/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1571\n",
      "Epoch 27/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1511\n",
      "Epoch 28/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1535\n",
      "Epoch 29/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1464\n",
      "Epoch 30/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1440\n",
      "Epoch 31/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1521\n",
      "Epoch 32/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1384\n",
      "Epoch 33/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1521\n",
      "Epoch 34/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1597\n",
      "Epoch 35/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1310\n",
      "Epoch 36/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1372\n",
      "Epoch 37/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1413\n",
      "Epoch 38/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1422\n",
      "Epoch 39/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1330\n",
      "Epoch 40/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1289\n",
      "Epoch 41/100\n",
      "68094/68094 [==============================] - 1s 21us/sample - loss: 0.1471\n",
      "Epoch 42/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1282\n",
      "Epoch 43/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1405\n",
      "Epoch 44/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1391\n",
      "Epoch 45/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1488\n",
      "Epoch 46/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1198\n",
      "Epoch 47/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1240\n",
      "Epoch 48/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1359\n",
      "Epoch 49/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1239\n",
      "Epoch 50/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1334\n",
      "Epoch 51/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1188\n",
      "Epoch 52/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1238\n",
      "Epoch 53/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1251\n",
      "Epoch 54/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1224\n",
      "Epoch 55/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1482\n",
      "Epoch 56/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1311\n",
      "Epoch 57/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1163\n",
      "Epoch 58/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1248\n",
      "Epoch 59/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1304\n",
      "Epoch 60/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1176\n",
      "Epoch 61/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1181\n",
      "Epoch 62/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1143\n",
      "Epoch 63/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1340\n",
      "Epoch 64/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1233\n",
      "Epoch 65/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1154\n",
      "Epoch 66/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1136\n",
      "Epoch 67/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1176\n",
      "Epoch 68/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1300\n",
      "Epoch 69/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1251\n",
      "Epoch 70/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1267\n",
      "Epoch 71/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1152\n",
      "Epoch 72/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1104\n",
      "Epoch 73/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1175\n",
      "Epoch 74/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1241\n",
      "Epoch 75/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1096\n",
      "Epoch 76/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1108\n",
      "Epoch 77/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1140\n",
      "Epoch 78/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1128\n",
      "Epoch 79/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1151\n",
      "Epoch 80/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1138\n",
      "Epoch 81/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1160\n",
      "Epoch 82/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1181\n",
      "Epoch 83/100\n",
      "68094/68094 [==============================] - 1s 20us/sample - loss: 0.1032\n",
      "Epoch 84/100\n",
      "68094/68094 [==============================] - 1s 21us/sample - loss: 0.1157\n",
      "Epoch 85/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1025\n",
      "Epoch 86/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1103\n",
      "Epoch 87/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1021\n",
      "Epoch 88/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1061\n",
      "Epoch 89/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1185\n",
      "Epoch 90/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1036\n",
      "Epoch 91/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1029\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1039\n",
      "Epoch 93/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1045\n",
      "Epoch 94/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1017\n",
      "Epoch 95/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1029\n",
      "Epoch 96/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1044\n",
      "Epoch 97/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1008\n",
      "Epoch 98/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1008\n",
      "Epoch 99/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.1034\n",
      "Epoch 100/100\n",
      "68094/68094 [==============================] - 1s 19us/sample - loss: 0.0937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0ad008e8d0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=x_normal.shape[1], activation='relu'))\n",
    "model.add(Dense(3, activation='relu')) # size to compress to\n",
    "\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(x_normal.shape[1])) # Multiple output neurons\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_normal_train,x_normal_train,verbose=1,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting an Anomaly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoding data that represents an attack. This higher error indicates an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Sample Normal Score (RMSE): 0.3306574651748449\n",
      "Insample Normal Score (RMSE): 0.32958383415984016\n",
      "Attack Underway Score (RMSE): 0.5283584940098758\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_normal_test)\n",
    "score1 = np.sqrt(metrics.mean_squared_error(pred,x_normal_test))\n",
    "pred = model.predict(x_normal)\n",
    "score2 = np.sqrt(metrics.mean_squared_error(pred,x_normal))\n",
    "pred = model.predict(x_attack)\n",
    "score3 = np.sqrt(metrics.mean_squared_error(pred,x_attack))\n",
    "print(f\"Out of Sample Normal Score (RMSE): {score1}\")\n",
    "print(f\"Insample Normal Score (RMSE): {score2}\")\n",
    "print(f\"Attack Underway Score (RMSE): {score3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
